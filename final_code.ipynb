{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive (already mounted, skipping this step as per output)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Unzip dataset (already done, skipping)\n",
        "from zipfile import ZipFile\n",
        "zip_file_path = '/content/drive/MyDrive/NIR-VIS-2.0.zip'\n",
        "extract_path = '/content/dataset/'\n",
        "with ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "     zip_ref.extractall(extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7zR3CPM7c9Lr",
        "outputId": "0525783d-4ccb-4ffc-8cf7-58b6b10cf38a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (Previous imports remain the same)\n",
        "from tensorflow.keras.mixed_precision import set_global_policy\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Layer\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Enable mixed precision training\n",
        "set_global_policy('mixed_float16')\n",
        "\n",
        "# Check GPU availability\n",
        "gpus = tf.config.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    print(f\"GPU available: {gpus}\")\n",
        "else:\n",
        "    print(\"No GPU available. Running on CPU.\")\n",
        "\n",
        "# Load paired images (already working)\n",
        "def load_paired_images(vis_dirs, nir_dirs, img_size=(128, 128)):\n",
        "    images = []\n",
        "    labels = []\n",
        "    for vis_dir, nir_dir in zip(vis_dirs, nir_dirs):\n",
        "        vis_paths = glob.glob(os.path.join(vis_dir, '**/*.bmp'), recursive=True)\n",
        "        nir_paths = glob.glob(os.path.join(nir_dir, '**/*.bmp'), recursive=True)\n",
        "        vis_by_id = {}\n",
        "        nir_by_id = {}\n",
        "        for p in vis_paths:\n",
        "            identity = os.path.basename(os.path.dirname(p))\n",
        "            if identity not in vis_by_id:\n",
        "                vis_by_id[identity] = []\n",
        "            vis_by_id[identity].append(p)\n",
        "        for p in nir_paths:\n",
        "            identity = os.path.basename(os.path.dirname(p))\n",
        "            if identity not in nir_by_id:\n",
        "                nir_by_id[identity] = []\n",
        "            nir_by_id[identity].append(p)\n",
        "        common_ids = set(vis_by_id.keys()).intersection(nir_by_id.keys())\n",
        "        print(f\"Found {len(common_ids)} common identities\")\n",
        "        for identity in common_ids:\n",
        "            vis_paths_id = vis_by_id[identity]\n",
        "            nir_paths_id = nir_by_id[identity]\n",
        "            for vis_path, nir_path in zip(vis_paths_id, nir_paths_id):\n",
        "                vis_img = cv2.imread(vis_path, cv2.IMREAD_COLOR)\n",
        "                nir_img = cv2.imread(nir_path, cv2.IMREAD_COLOR)\n",
        "                if vis_img is None or nir_img is None:\n",
        "                    continue\n",
        "                vis_img = cv2.resize(vis_img, img_size)\n",
        "                nir_img = cv2.resize(nir_img, img_size)\n",
        "                images.extend([vis_img, nir_img])\n",
        "                labels.extend([identity, identity])\n",
        "    images = np.array(images)\n",
        "    labels = np.array(labels)\n",
        "    print(f\"Loaded {len(images)} paired images with shape: {images.shape}\")\n",
        "    return images, labels\n",
        "\n",
        "# Load images\n",
        "vis_dirs = ['/content/dataset/s1/VIS_128x128']\n",
        "nir_dirs = ['/content/dataset/s1/NIR_128x128']\n",
        "images, labels = load_paired_images(vis_dirs, nir_dirs)\n",
        "\n",
        "# Filter classes with insufficient samples\n",
        "unique_labels, counts = np.unique(labels, return_counts=True)\n",
        "min_samples = 10\n",
        "valid_labels = unique_labels[counts >= min_samples]\n",
        "mask = np.isin(labels, valid_labels)\n",
        "images = images[mask]\n",
        "labels = labels[mask]\n",
        "print(f\"Reduced to {len(valid_labels)} classes with at least {min_samples} samples each.\")\n",
        "\n",
        "# Normalize images\n",
        "images = images.astype('float32') / 255.0\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n",
        "\n",
        "# Split into training and test sets (80-20 split)\n",
        "num_pairs = len(images) // 2\n",
        "pair_indices = np.arange(num_pairs)\n",
        "train_indices, test_indices = train_test_split(pair_indices, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert pair indices to image indices (each pair has 2 images)\n",
        "train_image_indices = np.concatenate([train_indices * 2, train_indices * 2 + 1])\n",
        "test_image_indices = np.concatenate([test_indices * 2, test_indices * 2 + 1])\n",
        "\n",
        "# Ensure indices are sorted to maintain order\n",
        "train_image_indices.sort()\n",
        "test_image_indices.sort()\n",
        "\n",
        "# Create training and test sets\n",
        "train_images = images[train_image_indices]\n",
        "train_labels = labels[train_image_indices]\n",
        "test_images = images[test_image_indices]\n",
        "test_labels = labels[test_image_indices]\n",
        "\n",
        "print(f\"Training set: {len(train_images)} images\")\n",
        "print(f\"Test set: {len(test_images)} images\")\n",
        "\n",
        "# Build embedding model (moved up for triplet generation)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(128, 128, 3))\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-20]:\n",
        "    layer.trainable = False\n",
        "\n",
        "embedding_model = Sequential([\n",
        "    base_model,\n",
        "    GlobalAveragePooling2D(),\n",
        "    Dense(128, activation=None, dtype='float32'),\n",
        "    tf.keras.layers.Dropout(0.3)\n",
        "])\n",
        "\n",
        "# Create a tf.data.Dataset for triplets with semi-hard negative mining\n",
        "def create_triplet_dataset(images, labels, embedding_model, batch_size=16, num_triplets=10000):\n",
        "    # Generate embeddings for all images\n",
        "    embeddings = embedding_model.predict(images)\n",
        "\n",
        "    def generate_triplets():\n",
        "        label_list = np.unique(labels)\n",
        "        if len(label_list) == 0:\n",
        "            raise ValueError(\"No labels available to create triplets.\")\n",
        "        for _ in range(num_triplets):\n",
        "            anchor_label = np.random.choice(label_list)\n",
        "            anchor_pos_indices = np.where(labels == anchor_label)[0]\n",
        "            if len(anchor_pos_indices) < 2:\n",
        "                continue\n",
        "            anchor_idx, pos_idx = np.random.choice(anchor_pos_indices, 2, replace=False)\n",
        "\n",
        "            # Compute distance between anchor and positive\n",
        "            anchor_emb = embeddings[anchor_idx]\n",
        "            pos_emb = embeddings[pos_idx]\n",
        "            pos_dist = np.sum(np.square(anchor_emb - pos_emb))\n",
        "\n",
        "            # Semi-hard negative mining: select a negative where pos_dist < neg_dist < pos_dist + margin\n",
        "            margin = 0.5\n",
        "            neg_candidates = np.where(labels != anchor_label)[0]\n",
        "            neg_dists = np.sum(np.square(embeddings[neg_candidates] - anchor_emb), axis=1)\n",
        "            semi_hard_mask = (neg_dists > pos_dist) & (neg_dists < pos_dist + margin)\n",
        "            valid_neg_indices = neg_candidates[semi_hard_mask]\n",
        "\n",
        "            if len(valid_neg_indices) == 0:\n",
        "                # Fallback to random negative if no semi-hard negatives are found\n",
        "                neg_idx = np.random.choice(np.where(labels != anchor_label)[0])\n",
        "            else:\n",
        "                neg_idx = np.random.choice(valid_neg_indices)\n",
        "\n",
        "            yield (images[anchor_idx], images[pos_idx], images[neg_idx])\n",
        "\n",
        "    dataset = tf.data.Dataset.from_generator(\n",
        "        generate_triplets,\n",
        "        output_signature=(\n",
        "            tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32),\n",
        "            tf.TensorSpec(shape=(128, 128, 3), dtype=tf.float32)\n",
        "        )\n",
        "    )\n",
        "\n",
        "    dataset = dataset.map(lambda anchor, pos, neg: tf.stack([anchor, pos, neg], axis=0),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.map(lambda triplet: (triplet, tf.constant(0.0, dtype=tf.float32)),\n",
        "                          num_parallel_calls=tf.data.AUTOTUNE)\n",
        "    dataset = dataset.shuffle(buffer_size=1000)\n",
        "    return dataset\n",
        "\n",
        "# Create the triplet dataset\n",
        "num_triplets = 10000\n",
        "dataset = create_triplet_dataset(train_images, train_labels, embedding_model, num_triplets=num_triplets)\n",
        "\n",
        "# Split into training and validation datasets\n",
        "triplet_count = 0\n",
        "for _ in dataset:\n",
        "    triplet_count += 1\n",
        "print(f\"Total number of triplets generated: {triplet_count}\")\n",
        "\n",
        "validation_split = 0.1\n",
        "val_size = int(triplet_count * validation_split)\n",
        "train_size = triplet_count - val_size\n",
        "\n",
        "train_dataset = dataset.take(train_size)\n",
        "val_dataset = dataset.skip(train_size)\n",
        "\n",
        "batch_size = 16\n",
        "train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# Define a custom layer for triplet processing\n",
        "class TripletLayer(Layer):\n",
        "    def __init__(self, embedding_model, **kwargs):\n",
        "        super(TripletLayer, self).__init__(**kwargs)\n",
        "        self.embedding_model = embedding_model\n",
        "\n",
        "    def call(self, inputs):\n",
        "        anchor = self.embedding_model(inputs[:, 0])\n",
        "        positive = self.embedding_model(inputs[:, 1])\n",
        "        negative = self.embedding_model(inputs[:, 2])\n",
        "        return tf.concat([anchor, positive, negative], axis=-1)\n",
        "\n",
        "# Build triplet model\n",
        "inputs = tf.keras.Input(shape=(3, 128, 128, 3))\n",
        "triplet_output = TripletLayer(embedding_model)(inputs)\n",
        "triplet_model = tf.keras.Model(inputs, triplet_output)\n",
        "\n",
        "# Define triplet loss\n",
        "def triplet_loss(y_true, y_pred, alpha=0.5):\n",
        "    anchor, positive, negative = y_pred[:, :128], y_pred[:, 128:256], y_pred[:, 256:]\n",
        "    pos_dist = tf.reduce_sum(tf.square(anchor - positive), axis=-1)\n",
        "    neg_dist = tf.reduce_sum(tf.square(anchor - negative), axis=-1)\n",
        "    loss = tf.maximum(pos_dist - neg_dist + alpha, 0.0)\n",
        "    return tf.reduce_mean(loss)\n",
        "\n",
        "# Compile model\n",
        "triplet_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss=triplet_loss)\n",
        "\n",
        "# Callbacks\n",
        "def lr_schedule(epoch):\n",
        "    lr = 1e-4\n",
        "    if epoch > 10:\n",
        "        lr *= 0.5\n",
        "    if epoch > 20:\n",
        "        lr *= 0.5\n",
        "    return lr\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "lr_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Train model\n",
        "history = triplet_model.fit(\n",
        "    train_dataset,\n",
        "    epochs=50,\n",
        "    validation_data=val_dataset,\n",
        "    callbacks=[early_stopping, lr_scheduler]\n",
        ")\n",
        "\n",
        "# Updated evaluation function with cosine similarity distribution\n",
        "def evaluate_embeddings(images, labels, num_pairs=1000):\n",
        "    embeddings = embedding_model.predict(images)\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    # Generate positive and negative pairs\n",
        "    similarities = []\n",
        "    pair_labels = []  # 1 for positive (same identity), 0 for negative (different identity)\n",
        "\n",
        "    # Positive pairs (same identity)\n",
        "    for _ in range(num_pairs // 2):\n",
        "        label = np.random.choice(unique_labels)\n",
        "        indices = np.where(labels == label)[0]\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "        idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
        "        emb1, emb2 = embeddings[idx1], embeddings[idx2]\n",
        "        sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "        similarities.append(sim)\n",
        "        pair_labels.append(1)\n",
        "\n",
        "    # Negative pairs (different identities)\n",
        "    for _ in range(num_pairs // 2):\n",
        "        label1, label2 = np.random.choice(unique_labels, 2, replace=False)\n",
        "        idx1 = np.random.choice(np.where(labels == label1)[0])\n",
        "        idx2 = np.random.choice(np.where(labels == label2)[0])\n",
        "        emb1, emb2 = embeddings[idx1], embeddings[idx2]\n",
        "        sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "        similarities.append(sim)\n",
        "        pair_labels.append(0)\n",
        "\n",
        "    # Convert to numpy arrays\n",
        "    similarities = np.array(similarities)\n",
        "    pair_labels = np.array(pair_labels)\n",
        "\n",
        "    # Plot histogram of similarities\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(similarities[pair_labels == 1], bins=30, alpha=0.5, label='Positive Pairs', color='blue')\n",
        "    plt.hist(similarities[pair_labels == 0], bins=30, alpha=0.5, label='Negative Pairs', color='red')\n",
        "    plt.title('Cosine Similarity Distribution')\n",
        "    plt.xlabel('Cosine Similarity')\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Test different thresholds\n",
        "    thresholds = np.arange(0.1, 1.0, 0.1)\n",
        "    accuracies = []\n",
        "    for thresh in thresholds:\n",
        "        predictions = (similarities > thresh).astype(int)\n",
        "        accuracy = np.mean(predictions == pair_labels)\n",
        "        accuracies.append(accuracy)\n",
        "        print(f\"Threshold {thresh:.1f}: Accuracy = {accuracy:.4f}\")\n",
        "\n",
        "    # Plot accuracy vs threshold\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(thresholds, accuracies, marker='o')\n",
        "    plt.title('Accuracy vs Cosine Similarity Threshold')\n",
        "    plt.xlabel('Threshold')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Return the best accuracy and threshold\n",
        "    best_idx = np.argmax(accuracies)\n",
        "    best_threshold = thresholds[best_idx]\n",
        "    best_accuracy = accuracies[best_idx]\n",
        "    print(f\"Best threshold: {best_threshold:.1f}, Best accuracy: {best_accuracy:.4f}\")\n",
        "    return best_accuracy, best_threshold\n",
        "\n",
        "# Evaluate on the test set\n",
        "best_accuracy, best_threshold = evaluate_embeddings(test_images, test_labels)\n",
        "\n",
        "# Plot training loss\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.plot(history.history['loss'], label='Train Loss')\n",
        "plt.plot(history.history['val_loss'], label='Val Loss')\n",
        "plt.title('Triplet Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "jOASje1H6g46",
        "outputId": "506d6bb8-5fd8-44f7-f267-241b178aad96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No GPU available. Running on CPU.\n",
            "Found 0 common identities\n",
            "Loaded 0 paired images with shape: (0,)\n",
            "Reduced to 0 classes with at least 10 samples each.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-4001392037.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0mnum_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mpair_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_pairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpair_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;31m# Convert pair indices to image indices (each pair has 2 images)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzL5zbHzE_ku",
        "outputId": "be184f4f-5a10-4dae-e520-305c043a8713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 15 10:13:02 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0             37W /   70W |    1410MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version\n",
        "!cat /usr/include/cudnn_version.h | grep CUDNN_MAJOR -A 2\n",
        "print(\"TensorFlow Version:\", tf.__version__)\n",
        "print(\"Built with CUDA:\", tf.test.is_built_with_cuda())\n",
        "print(\"GPU Available:\", tf.test.gpu_device_name())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3I-nfvyHpL9",
        "outputId": "d98b39ff-cf83-41ba-bd05-e816ce028745"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "#define CUDNN_MAJOR 9\n",
            "#define CUDNN_MINOR 2\n",
            "#define CUDNN_PATCHLEVEL 1\n",
            "--\n",
            "#define CUDNN_VERSION (CUDNN_MAJOR * 10000 + CUDNN_MINOR * 100 + CUDNN_PATCHLEVEL)\n",
            "\n",
            "/* cannot use constexpr here since this is a C-only file */\n",
            "TensorFlow Version: 2.18.0\n",
            "Built with CUDA: True\n",
            "GPU Available: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the existing evaluate_embeddings function with this\n",
        "def evaluate_embeddings(images, labels, spectrum_type=\"VIS\", num_pairs=1000):\n",
        "    embeddings = embedding_model.predict(images)\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    # Separate VIS and NIR indices (assuming pairs: even for VIS, odd for NIR)\n",
        "    if spectrum_type == \"VIS\":\n",
        "        image_indices = np.arange(0, len(images), 2)  # VIS as first in each pair\n",
        "    else:  # NIR\n",
        "        image_indices = np.arange(1, len(images), 2)  # NIR as second in each pair\n",
        "\n",
        "    embeddings = embeddings[image_indices]\n",
        "    labels = labels[image_indices]\n",
        "\n",
        "    # Generate pairs for ranking\n",
        "    ranks = []\n",
        "    for _ in range(num_pairs):\n",
        "        # Positive pair (same identity)\n",
        "        label = np.random.choice(unique_labels)\n",
        "        indices = np.where(labels == label)[0]\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "        idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
        "        emb1, emb2 = embeddings[idx1], embeddings[idx2]\n",
        "        sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "        # Negative pairs for ranking\n",
        "        neg_indices = np.where(labels != label)[0]\n",
        "        neg_sims = [np.dot(emb1, embeddings[neg_idx]) / (np.linalg.norm(emb1) * np.linalg.norm(embeddings[neg_idx])) for neg_idx in neg_indices]\n",
        "        all_sims = np.array([sim] + neg_sims)  # Positive sim + negative sims\n",
        "        all_indices = np.array([idx2] + neg_indices.tolist())\n",
        "        sorted_indices = all_indices[np.argsort(all_sims)[::-1]]  # Sort descending\n",
        "        rank = np.where(sorted_indices == idx2)[0][0] + 1  # Rank of positive match\n",
        "        ranks.append(rank)\n",
        "\n",
        "    # Compute CMC curve\n",
        "    max_rank = min(10, len(unique_labels))  # Limit to rank 10 or number of classes\n",
        "    cmc = np.zeros(max_rank)\n",
        "    for rank in ranks:\n",
        "        if rank <= max_rank:\n",
        "            cmc[rank - 1] += 1\n",
        "    cmc = cmc / len(ranks)  # Normalize to get rates\n",
        "\n",
        "    # Cumulative sum for CMC\n",
        "    cmc = np.cumsum(cmc)\n",
        "\n",
        "    # Plot CMC curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(range(1, max_rank + 1), cmc, marker='o')\n",
        "    plt.title(f'CMC Curve for {spectrum_type} Spectrum (Generated at 01:26 PM IST, June 23, 2025)')\n",
        "    plt.xlabel('Rank')\n",
        "    plt.ylabel('Identification Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return cmc\n",
        "\n",
        "# Add this after the training section to generate the graphs\n",
        "# Ensure images and labels are loaded correctly before this\n",
        "if len(images) > 0:  # Check if data is loaded\n",
        "    print(\"Generating CMC curve for VIS spectrum (Fig. 4)...\")\n",
        "    cmc_vis = evaluate_embeddings(images, labels, spectrum_type=\"VIS\")\n",
        "\n",
        "    print(\"Generating CMC curve for NIR spectrum (Fig. 5)...\")\n",
        "    cmc_nir = evaluate_embeddings(images, labels, spectrum_type=\"NIR\")\n",
        "else:\n",
        "    print(\"No images loaded. Please fix the dataset paths and rerun the notebook.\")"
      ],
      "metadata": {
        "id": "vBkz9bWg4jFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def evaluate_embeddings(images, labels, train_images, train_labels, spectrum_type=\"VIS\", num_pairs=1000):\n",
        "    # Generate embeddings for all images\n",
        "    embeddings = embedding_model.predict(images)\n",
        "    train_embeddings = embedding_model.predict(train_images)\n",
        "\n",
        "    # Separate VIS and NIR indices (assuming pairs: even for VIS, odd for NIR)\n",
        "    if spectrum_type == \"VIS\":\n",
        "        image_indices = np.arange(0, len(images), 2)  # VIS as first in each pair\n",
        "        train_indices = np.arange(0, len(train_images), 2)  # VIS training set\n",
        "    else:  # NIR\n",
        "        image_indices = np.arange(1, len(images), 2)  # NIR as second in each pair\n",
        "        train_indices = np.arange(1, len(train_images), 2)  # NIR training set\n",
        "\n",
        "    embeddings = embeddings[image_indices]\n",
        "    labels = labels[image_indices]\n",
        "    train_embeddings = train_embeddings[train_indices]\n",
        "    train_labels = train_labels[train_indices]\n",
        "\n",
        "    unique_labels = np.unique(labels)\n",
        "\n",
        "    # Generate pairs for CMC\n",
        "    ranks = []\n",
        "    for _ in range(num_pairs):\n",
        "        label = np.random.choice(unique_labels)\n",
        "        indices = np.where(labels == label)[0]\n",
        "        if len(indices) < 2:\n",
        "            continue\n",
        "        idx1, idx2 = np.random.choice(indices, 2, replace=False)\n",
        "        emb1, emb2 = embeddings[idx1], embeddings[idx2]\n",
        "        sim = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
        "\n",
        "        neg_indices = np.where(labels != label)[0]\n",
        "        neg_sims = [np.dot(emb1, embeddings[neg_idx]) / (np.linalg.norm(emb1) * np.linalg.norm(embeddings[neg_idx])) for neg_idx in neg_indices]\n",
        "        all_sims = np.array([sim] + neg_sims)\n",
        "        all_indices = np.array([idx2] + neg_indices.tolist())\n",
        "        sorted_indices = all_indices[np.argsort(all_sims)[::-1]]\n",
        "        rank = np.where(sorted_indices == idx2)[0][0] + 1\n",
        "        ranks.append(rank)\n",
        "\n",
        "    # Compute CMC curve\n",
        "    max_rank = min(10, len(unique_labels))\n",
        "    cmc = np.zeros(max_rank)\n",
        "    for rank in ranks:\n",
        "        if rank <= max_rank:\n",
        "            cmc[rank - 1] += 1\n",
        "    cmc = cmc / len(ranks)\n",
        "    cmc = np.cumsum(cmc)\n",
        "\n",
        "    # Plot CMC curve\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(range(1, max_rank + 1), cmc, marker='o')\n",
        "    plt.title(f'CMC Curve for {spectrum_type} Spectrum (Generated at 01:53 PM IST, June 23, 2025)')\n",
        "    plt.xlabel('Rank')\n",
        "    plt.ylabel('Identification Rate')\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    # Predict labels using nearest neighbor on train embeddings\n",
        "    predicted_labels = []\n",
        "    for emb in embeddings:\n",
        "        sims = np.array([np.dot(emb, train_emb) / (np.linalg.norm(emb) * np.linalg.norm(train_emb)) for train_emb in train_embeddings])\n",
        "        pred_idx = np.argmax(sims)\n",
        "        predicted_labels.append(train_labels[pred_idx])\n",
        "\n",
        "    predicted_labels = np.array(predicted_labels)\n",
        "    true_labels = labels\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    cm = confusion_matrix(true_labels, predicted_labels, labels=unique_labels)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=unique_labels, yticklabels=unique_labels)\n",
        "    plt.title(f'Confusion Matrix for {spectrum_type} Spectrum (Generated at 01:53 PM IST, June 23, 2025)')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.show()\n",
        "\n",
        "    return cmc, cm\n",
        "\n",
        "# Add this after the training section to generate graphs\n",
        "if len(images) > 0:  # Check if data is loaded\n",
        "    print(\"Generating CMC curve and confusion matrix for VIS spectrum (Fig. 4)...\")\n",
        "    cmc_vis, cm_vis = evaluate_embeddings(images, labels, train_images, train_labels, spectrum_type=\"VIS\")\n",
        "\n",
        "    print(\"Generating CMC curve and confusion matrix for NIR spectrum (Fig. 5)...\")\n",
        "    cmc_nir, cm_nir = evaluate_embeddings(images, labels, train_images, train_labels, spectrum_type=\"NIR\")\n",
        "else:\n",
        "    print(\"No images loaded. Please fix the dataset paths and rerun the notebook.\")"
      ],
      "metadata": {
        "id": "b1OJQGhW4rLJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}